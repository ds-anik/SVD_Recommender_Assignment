{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD_Recommender System\r\n",
    "\r\n",
    "This assignment has two main parts:\r\n",
    "\r\n",
    "    1. **PCA** : In this part the goal is to implement the dimensionality reduction technique *Principal Component Analysis (PCA)* to a very high dimensional data and apply visualization. Note that you are not allowed to use the built-in PCA API provided by the sklearn library. Instead you will be implementing from the scratch. Use the data in data/train.csv for generating the PCA. See the detailed intructions below.\r\n",
    "    \r\n",
    "    2. **Recommendation system** : In this part use SVD to get USVT decomposition on the data in train.csv to recommend the movies to the users in test.csv. The submissions.csv should contain user_id (from the test.csv) followed by recommended ratings for all movies.\r\n",
    "\r\n",
    "   For this task we use the  MovieLens dataset. The data is in train.csv.\r\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import sqrtm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-1a: Convert data to user-movie rating matrix\n",
    "    - Read the train.csv file and movies.dat file and use user_id and movie_id to create user-movie rating matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readMovieRatingData():\n",
    "    \n",
    "    train_df = pd.read_csv('./data/train.csv')\n",
    "    user_movie_df = pd.pivot_table(train_df, values='rating', index=['user_id'], columns=['movie_id'], fill_value=0)\n",
    "    user_movie_matrix = user_movie_df.to_numpy()  \n",
    "      \n",
    "    \n",
    "    return user_movie_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readMovieDeata():\n",
    "    \n",
    "    col_names = ['movie_id','title','genres']\n",
    "    movies = pd.read_table('./data/movies.dat' , sep= '::' , engine='python' , header= None , names= col_names)\n",
    "    movies['genres'] = movies['genres'].map(lambda x: x.split('|'))\n",
    "    \n",
    "    return movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We are going to compute PCA for movies so transpose the matrix using X=readMovieRatingData().T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-1b: Preprocessing\n",
    "Before implementing PCA you are required to perform some preprocessing steps:\n",
    "1. Mean normalization: Replace each feature/attribute, $x_{ji}$ with $x_j - \\mu_j$, In other words, determine the mean of each feature set, and then for each feature subtract the mean from the value, so we re-scale the mean to be 0 \n",
    "2. Feature scaling: If features have very different scales then scale make them comparable by altering the scale, so they all have a comparable range of values e.g. $x_{ji}$ is set to $(x_j - \\mu_j) / s_j$  Where $s_j$ is some measure of the range, so could be  $\\max(x_j) - \\min(x_j)$ or Standard deviation $stddev(x_j)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO We can see features have very different scales. So we apply feature scaling with Standard \n",
    "# deviation as measure of the range, using StandardScaler from scikit-learn\n",
    "\n",
    "def preprocessing(X_train):\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_train)\n",
    "    \n",
    "    return X_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-2: Covariance matrix\n",
    "Now the preprocessing is finished. Next, as explained in the lecture, you need to compute the covariance matrix https://en.wikipedia.org/wiki/Covariance_matrix. Given $n \\times m$ $n$ rows and $m$ columns matrix, a covariance matrix is an $n \\times n$ matrix given as below (sigma)\n",
    "$\\Sigma = \\frac{1}{m}\\sum{\\left(x^{i}\\right)\\times \\left(x^{i}\\right)^{T}}$\n",
    "You may use the \"numpy.cov\" function in numpy library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute X to covariance matrix cov_matrix.\n",
    "# This block is implemented in runPCA()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions for part 3, 4, and 5\n",
    "- getSVD() function is expected to return 3 values. For example: ```U, S, V = getSVD(cov_matrix)```\n",
    "- You can follow the skeleton below to have an idea on how the autograder's test calls your functions:\n",
    "```\n",
    "U, S, V = getSVD(cov_matrix)\n",
    "z = getKComponents(U, X, k)\n",
    "ratio = getVarianceRatio(z, U, X, k)\n",
    "```\n",
    "- Using the built-in PCA implementation in sklearn, the approximate X matrix can be obtained by function ```inverse_transform```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-3: SVD computation\n",
    "Now compute the SVD on the covariance matrix $SVD(\\Sigma)$. You may use the svd implementation in numpy.linalg.svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSVD(cov_matrix):\n",
    "    U, S, V = np.linalg.svd(cov_matrix, full_matrices=False)\n",
    "    return U, S, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-4: Compute PCA matrix (K dimensional)\n",
    "Now select the first $k$ columns from the matrix $U$ and multiply with $X$ to get $k$ dimensional representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getKComponents(U, X, K):\n",
    "    # implement matrix multiplication of first k columns of U * X\n",
    "    \n",
    "    U_reduced = U[: ,: K]\n",
    "    Z = X.dot(U_reduced)\n",
    "    \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-5: Compute Reconstruction Error\n",
    "Implement a function to compute the variance ratio (from reconstruction error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVarianceRatio(Z, U, X, K):\n",
    "    #Implement computation of reconstruction error\n",
    "    \n",
    "    U_reduced = U[: ,: K]\n",
    "    X_approx = Z.dot(U_reduced.T)\n",
    "    ratio = np.mean((X-X_approx).T.dot(X-X_approx))/np.mean(X.T.dot(X))  \n",
    "    \n",
    "    return ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-6: Scatter plot 2-dimensional PCA\n",
    "Using matplotlib plot the 2-dimensional scatter plot of the first 2 compoenents with y (movie genre from movies.dat file) as labels. Remember you are plotting movies in dimensions so you can label them with movie generes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_genre(x):\n",
    "    types = ['Action', 'Animation', 'Comedy', 'Drama', 'Romance', 'Thriller', 'Adventure', 'Sci-Fi', 'War', 'Documentary']\n",
    "    for t in types:\n",
    "        if t in x:\n",
    "            return t\n",
    "    return 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotFunction(PCA, movie_data):\n",
    "    \n",
    "    train = pd.read_csv('./data/train.csv')\n",
    "    movie_id_index = np.array(sorted(train['movie_id'].unique()))\n",
    "    df = pd.DataFrame(data=PCA, index= movie_id_index , columns=['PC1','PC2'])\n",
    "    df_new = df.merge(movie_data , how='inner' , left_index=True , right_on=['movie_id'])\n",
    "    df_new['mapped_genre'] =  df_new.genres.apply(lambda x: map_genre(x))\n",
    "    df_new.drop(['movie_id','title','genres'] , axis=1)\n",
    "    \n",
    "    fig = plt.figure(figsize = (14,8))\n",
    "    ax = fig.add_subplot(1,1,1) \n",
    "    ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "    ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "    ax.set_title('2 component PCA', fontsize = 20)\n",
    "    movie_types = ['Action', 'Animation', 'Comedy', 'Drama', 'Romance', 'Thriller', 'Adventure',\n",
    "               'Sci-Fi', 'War', 'Documentary' , 'Other']\n",
    "\n",
    "    for item in movie_types:\n",
    "        indices = df_new['mapped_genre'] == item\n",
    "        ax.scatter(df_new.loc[indices, 'PC1'] , df_new.loc[indices, 'PC2'] , s = 50)\n",
    "\n",
    "    ax.legend(movie_types)\n",
    "    ax.grid()\n",
    "    \n",
    "       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-7 Find best $K$\n",
    "Find the minimum value of $K$ with which the ratio between averaged squared projection error with total variation in data is less than 0.1% in other words we retain 99.9% of the variance. You can achieve this by repeating getKComponents with $K=1$ until the variance ratio is <= 0.1%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findBestK(initial, step):\n",
    "    \n",
    "    X = readMovieRatingData().T\n",
    "    X = preprocessing(X)\n",
    "    cov_matrix = np.cov(X.T)\n",
    "    U,_,_ = getSVD(cov_matrix)\n",
    "    \n",
    "    while(True):\n",
    "        \n",
    "        Z = getKComponents(U, X, initial)\n",
    "        ratio = getVarianceRatio(Z, U, X, initial)\n",
    "        \n",
    "        if (ratio <= 0.001):\n",
    "            break\n",
    "        else:\n",
    "            initial += step          \n",
    "           \n",
    "    return initial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-8: TSNE visualization\n",
    "Finally, having found an optimal $K$ use these components as an input data to another dimensionality reduction method called tSNE (https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) and reduce it to 2 dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, scatter plot the components given by the tSNE using matplotlib compare it to the earlier scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotFunction_tsne(tsne_pca_results, movie_data):\n",
    "    \n",
    "    train = pd.read_csv('./data/train.csv')\n",
    "    movie_id_index = np.array(sorted(train['movie_id'].unique()))\n",
    "    df = pd.DataFrame(data=tsne_pca_results, index= movie_id_index , columns=['PC1','PC2'])\n",
    "    df_new = df.merge(movie_data , how='inner' , left_index=True , right_on=['movie_id'])\n",
    "    df_new['mapped_genre'] =  df_new.genres.apply(lambda x: map_genre(x))\n",
    "    df_new.drop(['movie_id','title','genres'] , axis=1)\n",
    "    \n",
    "    df_new['tsne-pca-one'] = tsne_pca_results[:,0]\n",
    "    df_new['tsne-pca-two'] = tsne_pca_results[:,1]\n",
    "    \n",
    "    plt.figure(figsize=(16,10))\n",
    "\n",
    "    sns.scatterplot(\n",
    "        x=\"tsne-pca-one\", y=\"tsne-pca-two\",\n",
    "        hue=\"mapped_genre\",\n",
    "        palette=sns.color_palette(\"hls\", 11),\n",
    "        data=df_new,\n",
    "        legend=\"full\",\n",
    "        alpha=0.3,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-9: Recommender System\n",
    "## Part-9a\n",
    "    - In this part you will use the SVD to build your own recommender engine for the movielens data\n",
    "    - Use the user-movie rating matrix from the training data (data/train.csv) to decmopose it into USV^T or use getSVD function from earlier\n",
    "    - Convert the S to the diagonal matrix using np.diag\n",
    "    - Take k best components (extract kxk matrix). k value can be using PCA k_min you found earlier\n",
    "    - Take square root of S matrix using scipy.sqrtm package as s_squre_root\n",
    "    - Multiply take U_reduced (first k columns of U) with s_squre_root (nxk . kxk)\n",
    "    - Then multiply the result from previous step with V_reduced which is a kxm matrix and return a recommendation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRecommendationMatrix(U, s, V, k):\n",
    "    \n",
    "    s = np.diag(s)\n",
    "    s_reduced = s[0:k,0:k]\n",
    "    s_reduced = sqrtm(s_reduced)\n",
    "    \n",
    "    U_reduced = U[:,0:k]\n",
    "    V_reduced = V[0:k,:]\n",
    "    result = U_reduced.dot(s_reduced)\n",
    "    rec_matrix = result.dot(V_reduced)\n",
    "    \n",
    "    \n",
    "    return rec_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-9a\n",
    "    - Use the recommendation matrix from getRecommendationMatrix to recommend movies for the user-movie pairs in data/test.csv\n",
    "    - If user-movie pair exits in the training data, use the matrix value as the recommended rating, else take the mean of the ratings for that movie and recommend that\n",
    "    - Write the recommended ratings in submissions.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMovieRecommendations():\n",
    "    # Use user-movie rating matrix X from readMovieRatingData() earlier to compute SVD\n",
    "    # Read data/test.csv in a similar way and get the test dataframe\n",
    "    \n",
    "    train = pd.read_csv('./data/train.csv')\n",
    "    test = pd.read_csv('./data/test.csv')\n",
    "    train = pd.pivot_table(train, values='rating', index=['user_id'], columns=['movie_id'], fill_value=0)\n",
    "    movie_indices = train.columns.tolist()\n",
    "    \n",
    "    X = readMovieRatingData()\n",
    "    mask = X==0\n",
    "    masked_arr = np.ma.masked_array(X, mask)\n",
    "    item_means = np.mean(masked_arr, axis=0)\n",
    "    X = masked_arr.filled(item_means)\n",
    "    mean_matrix = np.tile(item_means, (X.shape[0],1))\n",
    "    X = X - mean_matrix\n",
    "    \n",
    "    U,S,V = getSVD(X)\n",
    "    rec_matrix = getRecommendationMatrix(U,S,V,17)\n",
    "    rec_matrix += mean_matrix\n",
    "    \n",
    "    masked_rec = np.ma.masked_array(rec_matrix, mask)\n",
    "    rec_means = np.mean(masked_rec, axis=0)\n",
    "    \n",
    "    \n",
    "    with open('submissions.csv', 'w') as sub_file:\n",
    "        sub_file.write('user_id,movie_id,rating'+ '\\n')\n",
    "        for _,row in test.iterrows():\n",
    "            \n",
    "            user = row['user_id']\n",
    "            item = row['movie_id']\n",
    "                        \n",
    "            if (item not in movie_indices):\n",
    "                pred_rating = 0\n",
    "                                  \n",
    "            elif (train.loc[user][item] != 0):\n",
    "                idx = movie_indices.index(item)\n",
    "                pred_rating = rec_matrix[int(user)-1][idx]\n",
    "                \n",
    "            else:\n",
    "                idx = movie_indices.index(item)\n",
    "                pred_rating = rec_means[idx]\n",
    "                \n",
    "            sub_file.write(str(user) + ',' + str(item) + ',' + str(pred_rating) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runPCA():\n",
    "    X = readMovieRatingData().T\n",
    "    movie_data = readMovieDeata()\n",
    "    X = preprocessing(X)\n",
    "    cov_matrix = np.cov(X.T)\n",
    "    U, S, V = getSVD(cov_matrix)\n",
    "    Z = getKComponents(U, X, 2)\n",
    "    ratio = getVarianceRatio(Z, U, X, 2)\n",
    "    #plotFunction(Z, movie_data)                      # uncomment for showing the plot for part 6\n",
    "    K_best = findBestK(2, 2)\n",
    "    \n",
    "    pca = PCA(n_components=K_best)                    # TSNE Plot part8\n",
    "    z_pca = pca.fit_transform(X)\n",
    "    \n",
    "    tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
    "    tsne_pca_results = tsne.fit_transform(z_pca)\n",
    "    #plotFunction_tsne(tsne_pca_results, movie_data)       # uncomment for showing the plot for part 8\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    runPCA()\n",
    "    getMovieRecommendations()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('tensorflow': conda)",
   "name": "tensorflow"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}